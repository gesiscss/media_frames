{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for uploading the data and looking at it (analysis of columns, missing data, etc.)\n",
    "The path to the data is: <br/>\n",
    "http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/us_part1 <br/>\n",
    "http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/us_part2 <br/>\n",
    "http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/uk <br/>\n",
    "And the html files are at http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/tmp/Netquest/downloads/us/2019-04-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = '../../iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/'\n",
    "news = pd.DataFrame()\n",
    "DATA_FILES = ['us_part1/newspaper.json', 'us_part2/newspaper.json', 'uk/newspaper.json']\n",
    "for filepath in DATA_FILES:\n",
    "    current_news = pd.read_json(PATH_TO_DATA_FOLDER+filepath, lines=True)\n",
    "    current_news['country'] = filepath[:2]\n",
    "    news = news.append(current_news)\n",
    "    \n",
    "\n",
    "news.reset_index(inplace=True, drop=True)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[(news['meta_lang'] == 'en') | (news['meta_lang'].isnull())] #select only articles with 'en' meta_lang or with empty\n",
    "news = news[news['htmlfile'].str.contains(\"2019-04-08\")] #select only articles for 2019-04-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['domain'] = news['url'].apply (lambda row: tldextract.extract(row).domain)\n",
    "news['subdomain'] = news['url'].apply (lambda row: tldextract.extract(row).subdomain)\n",
    "news['suffix'] = news['url'].apply (lambda row: tldextract.extract(row).suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of unique URLs and domains  per country:')\n",
    "news_by_countries = news.groupby('country')\n",
    "news_by_countries.agg({\"url\": \"nunique\", \"domain\": \"nunique\",\"subdomain\": \"nunique\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_by_domains = news.groupby(['country', 'domain'])\n",
    "urls_by_domains = countries_by_domains.agg({\"url\":\"nunique\"}).sort_values('url', ascending=False)\n",
    "urls_by_domains.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_by_domains['fraction'] = urls_by_domains['url'].apply(lambda x: x/11803) #please change the number here to the corresponding number of total unique URLs\n",
    "us_domains = urls_by_domains[urls_by_domains['country'] == 'us'][:30] #change a country to either 'uk' or 'us'\n",
    "us_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "k_top = urls_by_domains[urls_by_domains['country'] == 'us'][:k]\n",
    "print(f'For top {k} the fraction is ', k_top['fraction'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting html files for US domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(soup):\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text\n",
    "\n",
    "PATH_TO_HTMLS = \"../../iboeckmann/webtracking_analysis/parsing/tmp/Netquest/downloads/us/2019-04-08/web_page/\"\n",
    "htmls_dict = {}\n",
    "for domain in k_top['domain']:\n",
    "    df = news.loc[news['domain'] == domain]\n",
    "    if int(us_domains[us_domains['domain'] == domain]['url']) <15:\n",
    "        n = int(us_domains[us_domains['domain'] == domain]['url'])\n",
    "        max_count = 1\n",
    "    else:\n",
    "        n = 15\n",
    "        max_count = 3\n",
    "    htmls_dict[domain] = {'count_not_found': [], 'count_found': [], 'count_contain_text': [], 'text': []}\n",
    "    count = 0\n",
    "    while count < max_count:\n",
    "        htmls_dict[domain]['count_found'].append(0)\n",
    "        htmls_dict[domain]['count_contain_text'].append(0)\n",
    "        htmls_dict[domain]['count_not_found'].append(0)\n",
    "        sample_df = df['htmlfile'].sample(n = n)\n",
    "        for sample in sample_df:\n",
    "            try:\n",
    "                temp = open(PATH_TO_HTMLS+sample, \"r\").read()\n",
    "                htmls_dict[domain]['count_found'][count] += 1\n",
    "                soup = BeautifulSoup(temp)\n",
    "                text = get_text(soup)\n",
    "                if len(text) > 2000:\n",
    "                    htmls_dict[domain]['count_contain_text'][count] += 1\n",
    "                    htmls_dict[domain]['text'].append(text)\n",
    "            except:\n",
    "                htmls_dict[domain]['count_not_found'][count] += 1\n",
    "          \n",
    "        count += 1\n",
    "        \n",
    "    htmls_dict[domain]['count_found'] = mean(htmls_dict[domain]['count_found'])\n",
    "    htmls_dict[domain]['count_not_found'] = mean(htmls_dict[domain]['count_not_found'])\n",
    "    htmls_dict[domain]['count_contain_text'] = mean(htmls_dict[domain]['count_contain_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls_df = pd.DataFrame(htmls_dict).transpose()\n",
    "htmls_df['found_pages'] = htmls_df.apply(lambda x: 1 - x['count_not_found']/15, axis=1)\n",
    "htmls_df['text_pages'] = htmls_df.apply(lambda x: x['count_contain_text']/x['count_found'] if x['count_found']!=0 else 0, axis=1)\n",
    "htmls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in news.columns:\n",
    "    print(column)\n",
    "    print('Number of null elements: ', len(news[news[column].isnull()])) #number of null entries\n",
    "    print(news[~news[column].isnull()][column][0:10]) #example of non-empty elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_keywords are lists and just .isnull() does not capture empty elements\n",
    "keywords = news[~news['meta_keywords'].isnull()]\n",
    "keywords[keywords.meta_keywords.map(len)>1]['meta_keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the distribution of the languages\n",
    "news.meta_lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.loc[10029, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.title.value_counts() #this will show repeated titles for non-existing or service pages pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['url'][222222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
