{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from advanced_pca import CustomPCA\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataframe with pre-processed tokens\n",
    "df = pd.read_csv(\"preprocessed_results/mediacloud_parsed_corona_df.csv\")\n",
    "df = df[~df.Text.isnull()]\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval) #transforming string of tokens to list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding 500 most frequent tokens\n",
    "flatten_tokens = [token for sublist in df['tokens'].tolist() for token in sublist]\n",
    "counter_tokens = Counter(flatten_tokens)\n",
    "most_frequent = counter_tokens.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving them to csv file\n",
    "with open('most_frequent_tokens.csv', \"w\") as the_file:\n",
    "    csv.register_dialect(\"custom\", delimiter=\",\", skipinitialspace=True)\n",
    "    writer = csv.writer(the_file, dialect=\"custom\")\n",
    "    for tup in most_frequent:\n",
    "        writer.writerow(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually excluding from this list all names of persons, locations, and organisations, as well as all dates and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.read_csv('most_frequent_tokens_cleaned_v2.csv', header=None, names=['token', 'frequency'])\n",
    "tokens['tfidf'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "cv = CountVectorizer(analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "data = cv.fit_transform(df['tokens'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "# create dictionary to find a tfidf word each word\n",
    "word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, score in word2tfidf.items():\n",
    "    if word in tokens['token'].tolist():\n",
    "        tokens.loc[tokens['token']==word, 'tfidf'] = score\n",
    "        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the frequent tokens for each document\n",
    "def filter_tokens(all_tokens):\n",
    "#     print(np.array([tokens.loc[tokens['token'] == token, 'tfidf'].values[0] for token in all_tokens if token in tokens['token'].tolist()]))\n",
    "    return np.array([tokens.loc[tokens['token'] == token, 'token'].values[0] for token in all_tokens if token in tokens['token'].tolist()])\n",
    "\n",
    "# df['filtered_tokens'] = df['tokens'].apply(filter_tokens)\n",
    "tokens_transformed = df['tokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_frequent = np.zeros((len(df['tokens']),len(tokens)))\n",
    "for i in range(len(tokens_transformed)):\n",
    "    for j, row in tokens.iterrows():\n",
    "        if row['token'] in tokens_transformed[i]:\n",
    "            tfidf_frequent[i,j] = row['tfidf']\n",
    "\n",
    "tfidf_frequent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(  #initiating a tfidf vectorizer from list of tokens\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) \n",
    "\n",
    "tokens_transformed = tfidf.fit_transform(df['filtered_tokens']) #fit and transform to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_frequent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tokens_transformed).shape #= [np.array(token_list) for token_list in tokens_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_all,kmo_model=calculate_kmo(np.array(tfidf_frequent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = np.zeros((tfidf_frequent.shape[0], len(kmo_all)))\n",
    "for i in range(len(kmo_all)):\n",
    "    if kmo_all[i] > 0.5:  #keeping only those that have kmo over 0.5\n",
    "        features_pca[i] = tfidf_frequent[i]\n",
    "    \n",
    "print(len(features_pca), tfidf_frequent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PCA on the filtered tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_pca_scaled = scaler.fit_transform(features_pca)\n",
    "\n",
    "pca_results = {'Num_of_components': [],\n",
    "                'Explained_variance': [],\n",
    "               'Terms':[]\n",
    "                }\n",
    "for n in range (3, 21):\n",
    "    pca_model = (CustomPCA(n_components=n)\n",
    "                    .fit(features_pca_scaled))\n",
    "    pca_results['Num_of_components'].append(n)\n",
    "    pca_results['Explained_variance'].append(sum(pca_model.explained_variance_ratio_))\n",
    "    all_terms = []\n",
    "    for i in range(n):\n",
    "        scores = [score for score in pca_model.components_[i].round(1) if score>0.1 or score<-0.1]\n",
    "#             tokens_sign = (pca_model.components_[i].round(1)>0.1) or (pca_model.components_[i].round(1)<-0.1)\n",
    "        terms = tokens.token[(pca_model.components_[i].round(1)>0.1) | (pca_model.components_[i].round(1)<-0.1)]\n",
    "        all_terms.append(list(zip(terms, scores)))\n",
    "    pca_results['Terms'].append(all_terms)\n",
    "    \n",
    "pca_results_df = pd.DataFrame(pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_df['Terms'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_df.to_csv('results/mediacloud_pca_results_shortlist.csv')\n",
    "pca_results_df.plot.line(x='Num_of_components', y='Explained_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_df['Terms'].to_csv('pca_terms.csv')\n",
    "print(pca_results_df['Terms'][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_pca = PCA().fit(features_pca_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x_values = range(1, cummulative_pca.n_components_+1)\n",
    "ax.plot(x_values, cummulative_pca.explained_variance_ratio_, lw=2, label='explained variance')\n",
    "ax.plot(x_values, np.cumsum(cummulative_pca.explained_variance_ratio_), lw=2, label='cumulative explained variance')\n",
    "ax.set_title('PCA on filtered tokens : explained variance of components')\n",
    "ax.set_xlabel('principal component')\n",
    "ax.set_ylabel('explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df['tokens'], min_count=3, threshold=50) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "df['bigrams'] = make_bigrams(df['tokens'])\n",
    "df['bigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_bigrams = [token for sublist in df['bigrams'].tolist() for token in sublist]\n",
    "counter_bigrams = Counter(flatten_bigrams)\n",
    "most_frequent = counter_bigrams.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving them to csv file\n",
    "with open('most_frequent_tokens_bigrams.csv', \"w\") as the_file:\n",
    "    csv.register_dialect(\"custom\", delimiter=\",\", skipinitialspace=True)\n",
    "    writer = csv.writer(the_file, dialect=\"custom\")\n",
    "    for tup in most_frequent:\n",
    "        writer.writerow(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bigrams = pd.read_csv('most_frequent_tokens_bigrams.csv', header=None, names=['token', 'frequency'])\n",
    "tokens_bigrams['tfidf'] = 0\n",
    "cv = CountVectorizer(analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "data = cv.fit_transform(df['bigrams'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "# create dictionary to find a tfidf word each word\n",
    "word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, score in word2tfidf.items():\n",
    "    if word in tokens_bigrams['token'].tolist():\n",
    "        tokens_bigrams.loc[tokens_bigrams['token']==word, 'tfidf'] = score\n",
    "        \n",
    "tokens_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_bigrams(all_tokens):\n",
    "#     print(np.array([tokens.loc[tokens['token'] == token, 'tfidf'].values[0] for token in all_tokens if token in tokens['token'].tolist()]))\n",
    "    return np.array([tokens_bigrams.loc[tokens_bigrams['token'] == token, 'token'].values[0] for token in all_tokens if token in tokens_bigrams['token'].tolist()])\n",
    "\n",
    "# df['filtered_tokens'] = df['tokens'].apply(filter_tokens)\n",
    "tokens_transformed = df['bigrams'].apply(filter_tokens_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_frequent_bigrams = np.zeros((len(df['bigrams']),len(tokens_bigrams)))\n",
    "for i in range(len(tokens_transformed)):\n",
    "    for j, row in tokens_bigrams.iterrows():\n",
    "        if row['token'] in tokens_transformed[i]:\n",
    "            tfidf_frequent_bigrams[i,j] = row['tfidf']\n",
    "\n",
    "tfidf_frequent_bigrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_all_bi,kmo_model_bi=calculate_kmo(np.array(tfidf_frequent_bigrams))\n",
    "kmo_model_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bigrams = np.zeros((tfidf_frequent_bigrams.shape[0], len(kmo_all_bi)))\n",
    "for i in range(len(kmo_all_bi)):\n",
    "    if kmo_all_bi[i] > 0.5:  #keeping only those that have kmo over 0.5\n",
    "        features_bigrams[i] = tfidf_frequent_bigrams[i]\n",
    "    \n",
    "print(len(features_bigrams), tfidf_frequent_bigrams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_bi_scaled = scaler.fit_transform(features_bigrams)\n",
    "\n",
    "pca_results_bi = {'Num_of_components': [],\n",
    "                'Explained_variance': [],\n",
    "               'Terms':[]\n",
    "                }\n",
    "for n in range (3, 21):\n",
    "    pca_model = (CustomPCA(n_components=n)\n",
    "                    .fit(features_bi_scaled))\n",
    "    pca_results_bi['Num_of_components'].append(n)\n",
    "    pca_results_bi['Explained_variance'].append(sum(pca_model.explained_variance_ratio_))\n",
    "    all_terms = []\n",
    "    for i in range(n):\n",
    "        scores = [score for score in pca_model.components_[i].round(1) if score>0.1 or score<-0.1]\n",
    "#             tokens_sign = (pca_model.components_[i].round(1)>0.1) or (pca_model.components_[i].round(1)<-0.1)\n",
    "        terms = tokens_bigrams.token[(pca_model.components_[i].round(1)>0.1) | (pca_model.components_[i].round(1)<-0.1)]\n",
    "        all_terms.append(list(zip(terms, scores)))\n",
    "    pca_results_bi['Terms'].append(all_terms)\n",
    "    \n",
    "pca_results_bi_df = pd.DataFrame(pca_results_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_bi_df['Terms'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_bi_df.to_csv('results/mediacloud_pca_bigrams_results_shortlist.csv')\n",
    "pca_results_bi_df.plot.line(x='Num_of_components', y='Explained_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_pca = PCA().fit(features_bi_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x_values = range(1, cummulative_pca.n_components_+1)\n",
    "ax.plot(x_values, cummulative_pca.explained_variance_ratio_, lw=2, label='explained variance')\n",
    "ax.plot(x_values, np.cumsum(cummulative_pca.explained_variance_ratio_), lw=2, label='cumulative explained variance')\n",
    "ax.set_title('PCA on filtered tokens : explained variance of components')\n",
    "ax.set_xlabel('principal component')\n",
    "ax.set_ylabel('explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
