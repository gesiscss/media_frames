{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ncov-or-cov-19-or-covid-o-all-story-urls-20200628121924.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only rows where files exist\n",
    "\n",
    "files = []\n",
    "for file in os.listdir('Mediacloud_parsed/'):\n",
    "    files.append(file[:-4])\n",
    "\n",
    "df['stories_id'] = df['stories_id'].astype('str')\n",
    "df = df[df['stories_id'].isin(files)]\n",
    "df['Text'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading files and adding text to the dataframe\n",
    "\n",
    "for file in os.listdir('Mediacloud_parsed/'):\n",
    "    if file.endswith('txt'):\n",
    "        with open('Mediacloud_parsed//' + file, 'r') as text:\n",
    "            data = text.read().replace('\\n', '')\n",
    "            df.loc[df['stories_id'] == file[:-4], 'Text'] = data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #separate english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating text length and briefly describing it\n",
    "\n",
    "df['text_len'] = df['Text'].apply(lambda x: len(str(x).split()) if x else np.nan)\n",
    "print(df['text_len'].describe())\n",
    "df.hist(column='text_len', bins=15, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('mediacloud_text_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataframes into 2: with text longer than 100 words and shorter for manual analysis\n",
    "df_short = df[df['text_len'] <= 100].reset_index()\n",
    "print(df_short.shape)\n",
    "df_medium = df[(df['text_len'] > 100) & (df['text_len'] <= 300)].reset_index()\n",
    "print(df_medium.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_short.sample(n=50)['Text'].to_csv('mc_short_texts')\n",
    "df_medium.sample(n=50)['Text'].to_csv('mc_medium_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting those rows where more than 100 words are present in the text\n",
    "\n",
    "df = df[df['text_len'] > 100].reset_index() #take a sample of articles of more than 100 and less than 100 and manually annotate if they make sense\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mediacloud_text_df.csv')\n",
    "df = df[~df['Text'].isnull()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stemming vs. Lemmatization: </b> <br/>\n",
    "stemming was found to\n",
    "reduce model fit, negligibly affect topic coherence, and negligibly or negatively affect\n",
    "model consistency across random initializations (Schofield and Mimno, 2016). In light of these results, authors recommended refraining from stemming the corpus as a pre-processing step and instead stemming the top-m word lists as a post-processing step, as needed. <br/>\n",
    "TO-DO: <br/>\n",
    "try a model with lemmatized and stemmed tokens as well as without it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the text and returning tokens\n",
    "\n",
    "def remove(tokens): \n",
    "    pattern = '[0-9]'\n",
    "    new_tokens = [re.sub(pattern, '', i) for i in tokens]  #removing numbers from tokens\n",
    "    removetable = str.maketrans('', '', '’“”–')  \n",
    "    new_tokens = [s.translate(removetable) for s in new_tokens] #removing special characters\n",
    "    return [x for x in new_tokens if len(x)>1]  #removing tokens with length 1 or empty\n",
    "\n",
    "\n",
    "def text_processing(input_str):\n",
    "    input_lower = input_str.lower()\n",
    "    input_punctutation = input_lower.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()\n",
    "    input_tokens = nltk.word_tokenize(input_punctutation)\n",
    "    input_stopwords = [i for i in input_tokens if not i in stop_words]\n",
    "    lemmatizer=WordNetLemmatizer()  #lemmatization vs. stemming\n",
    "    input_lemmatized = [lemmatizer.lemmatize(word) for word in input_stopwords]\n",
    "    input_clean = remove(input_lemmatized)\n",
    "    return (input_clean)\n",
    "    \n",
    "df['tokens'] = df['Text'].apply(lambda x: text_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'].sample(n=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the frequency of words and return freq of corona-related terms\n",
    "\n",
    "def wordListToFreqDict(wordlist, terms):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    res = dict(list(zip(wordlist,wordfreq)))\n",
    "    return [(term, res[term]) for term in terms if term in res.keys()]\n",
    "\n",
    "with open('corona_terms.txt', 'r') as corona_terms:\n",
    "    terms = corona_terms.read().replace('\\n', ' ').split(' ')\n",
    "\n",
    "df['corona_terms'] = df['tokens'].apply(lambda x: wordListToFreqDict(x, terms))\n",
    "df['corona_freq'] = df['corona_terms'].apply(lambda x: sum([item[1] for item in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corona_freq'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corona = df[df['corona_freq'] >=3]\n",
    "df_corona.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corona_terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corona.to_csv('mediacloud_parsed_corona_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
