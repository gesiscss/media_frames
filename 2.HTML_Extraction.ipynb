{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = '../../iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/'\n",
    "news = pd.DataFrame()\n",
    "DATA_FILES = ['us_part1/newspaper.json', 'us_part2/newspaper.json', 'uk/newspaper.json']\n",
    "for filepath in DATA_FILES:\n",
    "    current_news = pd.read_json(PATH_TO_DATA_FOLDER+filepath, lines=True)\n",
    "    current_news['country'] = filepath[:2]\n",
    "    news = news.append(current_news)\n",
    "    \n",
    "\n",
    "news.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "news = news[(news['meta_lang'] == 'en') | (news['meta_lang'].isnull())] #select only articles with 'en' meta_lang or with empty\n",
    "def extract_domain(row):\n",
    "    domain = row.split('/')[0]\n",
    "    domain = domain.replace('www.', '')\n",
    "    return domain\n",
    "\n",
    "news['domain'] = news['url'].apply(lambda row: extract_domain(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(soup):\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text\n",
    "\n",
    "PATH_TO_HTMLS = \"../../iboeckmann/webtracking_analysis/parsing/tmp/Netquest/downloads/us/2019-04-08/web_page/\"\n",
    "htmls_dict = {}\n",
    "for domain in k_top['domain']:\n",
    "    df = news.loc[news['domain'] == domain]\n",
    "    if int(us_domains[us_domains['domain'] == domain]['url']) <15:\n",
    "        n = int(us_domains[us_domains['domain'] == domain]['url'])\n",
    "        max_count = 1\n",
    "    else:\n",
    "        n = 15\n",
    "        max_count = 3\n",
    "    htmls_dict[domain] = {'count_not_found': [], 'count_found': [], 'count_contain_text': [], 'text': []}\n",
    "    count = 0\n",
    "    while count < max_count:\n",
    "        htmls_dict[domain]['count_found'].append(0)\n",
    "        htmls_dict[domain]['count_contain_text'].append(0)\n",
    "        htmls_dict[domain]['count_not_found'].append(0)\n",
    "        sample_df = df['htmlfile'].sample(n = n)\n",
    "        for sample in sample_df:\n",
    "            try:\n",
    "                temp = open(PATH_TO_HTMLS+sample, \"r\").read()\n",
    "                htmls_dict[domain]['count_found'][count] += 1\n",
    "                soup = BeautifulSoup(temp)\n",
    "                text = get_text(soup)\n",
    "                if len(text) > 2000:\n",
    "                    htmls_dict[domain]['count_contain_text'][count] += 1\n",
    "                    htmls_dict[domain]['text'].append(text)\n",
    "            except:\n",
    "                htmls_dict[domain]['count_not_found'][count] += 1\n",
    "          \n",
    "        count += 1\n",
    "        \n",
    "    htmls_dict[domain]['count_found'] = mean(htmls_dict[domain]['count_found'])\n",
    "    htmls_dict[domain]['count_not_found'] = mean(htmls_dict[domain]['count_not_found'])\n",
    "    htmls_dict[domain]['count_contain_text'] = mean(htmls_dict[domain]['count_contain_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls_df = pd.DataFrame(htmls_dict).transpose()\n",
    "htmls_df['found_pages'] = htmls_df.apply(lambda x: 1 - x['count_not_found']/15, axis=1)\n",
    "htmls_df['text_pages'] = htmls_df.apply(lambda x: x['count_contain_text']/x['count_found'] if x['count_found']!=0 else 0, axis=1)\n",
    "htmls_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
