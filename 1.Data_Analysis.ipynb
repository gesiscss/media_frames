{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for uploading the data and looking at it (analysis of columns, missing data, etc.)\n",
    "The path to the data is: <br/>\n",
    "http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/us_part1 <br/>\n",
    "http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/us_part2 <br/>\n",
    "http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/uk <br/>\n",
    "And the html files are at http://194.95.75.11/home/iboeckmann/webtracking_analysis/parsing/tmp/Netquest/downloads/us/2019-04-08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from statistics import mean \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = '../../iboeckmann/webtracking_analysis/parsing/data/archive/parsed_news-websites/'\n",
    "news = pd.DataFrame()\n",
    "DATA_FILES = ['us_part1/newspaper.json', 'us_part2/newspaper.json', 'uk/newspaper.json']\n",
    "for filepath in DATA_FILES:\n",
    "    current_news = pd.read_json(PATH_TO_DATA_FOLDER+filepath, lines=True)\n",
    "    current_news['country'] = filepath[:2]\n",
    "    news = news.append(current_news)\n",
    "    \n",
    "\n",
    "news.reset_index(inplace=True, drop=True)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[(news['meta_lang'] == 'en') | (news['meta_lang'].isnull())] #select only articles with 'en' meta_lang or with empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with media domains\n",
    "domains_path = '../../iboeckmann/webtracking_analysis/parsing/data/domaincodes/domaincodes.csv'\n",
    "news_domains = pd.read_csv(domains_path)\n",
    "news_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(row):\n",
    "    domain = row.split('/')[0]\n",
    "    domain = domain.replace('www.', '')\n",
    "    return domain\n",
    "\n",
    "news['domain'] = news['url'].apply(lambda row: extract_domain(row))\n",
    "news['domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#domain_mask \n",
    "media_portals = news[news['domain'].isin(news_domains['domain'].values)]\n",
    "media_portals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals['htmlfile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of unique URLs and domains  per country:')\n",
    "news_by_countries = media_portals.groupby('country')\n",
    "news_by_countries.agg({\"url\": \"nunique\", \"domain\": \"nunique\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_by_domains = media_portals.groupby(['country', 'domain'])\n",
    "urls_by_domains = countries_by_domains.agg({\"url\":\"nunique\"}).sort_values('url', ascending=False)\n",
    "urls_by_domains.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_by_domains['fraction'] = urls_by_domains['url'].apply(lambda x: x/52721) #please change the number here to the corresponding number of total unique URLs\n",
    "us_domains = urls_by_domains[urls_by_domains['country'] == 'us'][:30] #change a country to either 'uk' or 'us'\n",
    "us_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "k_top = urls_by_domains[urls_by_domains['country'] == 'us'][:k]\n",
    "print(f'For top {k} the fraction is ', k_top['fraction'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagetext analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals['text_len'] = media_portals['text'].apply(lambda x: len(str(x).split()) if x else np.nan)\n",
    "media_portals_clean = media_portals[~media_portals['text_len'].isnull()].copy() #omitting those rows where text is empty\n",
    "media_portals_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = media_portals_clean['title'].value_counts() #show how many titles are repeating\n",
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list[title_list<5] #are there unique titles that repeat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_clean[media_portals_clean['title'] == 'Schoolboy finds lost piece of Glasgow\\'s Govan Stones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_unique = media_portals_clean.drop_duplicates(subset =\"text\", \n",
    "                     keep = False)\n",
    "media_portals_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = media_portals_unique['title'].value_counts()\n",
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_portals = media_portals_unique[media_portals_unique['title'].isin(title_list[title_list>7].index)] #getting only pages with frequent articles\n",
    "frequent_portals['text_len'].describe() #how long is the text for such pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = media_portals_unique[media_portals_unique['title'].isin(title_list[title_list<=7].index)]\n",
    "df_1['text_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(frequent_portals.loc[frequent_portals['text_len']>132, 'url'])\n",
    "frequent_portals.loc[767166, 'url'] #has text_len over 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(frequent_portals.loc[frequent_portals['text_len']>2000]) #those that have the same title but still long text\n",
    "fr_portals = list(frequent_portals.index.values)\n",
    "fr_portals = [elem for elem in fr_portals if elem not in [883485, 887186, 899170, 932244, 988452]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frequent_portals.loc[frequent_portals['text_len']>500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_unique = media_portals_unique.drop(fr_portals) #dropping urls with frequent titles\n",
    "media_portals_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_unique['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_unique['text_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_unique.hist(column='text_len', by='country', layout=(2,1), bins=15, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_portals_unique[media_portals_unique['text_len']>2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in news.columns:\n",
    "    print(column)\n",
    "    print('Number of null elements: ', len(news[news[column].isnull()])) #number of null entries\n",
    "    print(news[~news[column].isnull()][column][0:10]) #example of non-empty elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_keywords are lists and just .isnull() does not capture empty elements\n",
    "keywords = news[~news['meta_keywords'].isnull()]\n",
    "keywords[keywords.meta_keywords.map(len)>1]['meta_keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the distribution of the languages\n",
    "news.meta_lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.loc[10029, 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.title.value_counts() #this will show repeated titles for non-existing or service pages pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['url'][222222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
