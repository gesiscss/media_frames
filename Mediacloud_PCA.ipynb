{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from advanced_pca import CustomPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataframe with pre-processed tokens\n",
    "df = pd.read_csv(\"preprocessed_results/mediacloud_parsed_corona_df.csv\")\n",
    "df = df[~df.Text.isnull()]\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval) #transforming string of tokens to list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding 500 most frequent tokens\n",
    "flatten_tokens = [token for sublist in df['tokens'].tolist() for token in sublist]\n",
    "counter_tokens = Counter(flatten_tokens)\n",
    "most_frequent = counter_tokens.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving them to csv file\n",
    "with open('most_frequent_tokens.csv', \"w\") as the_file:\n",
    "    csv.register_dialect(\"custom\", delimiter=\",\", skipinitialspace=True)\n",
    "    writer = csv.writer(the_file, dialect=\"custom\")\n",
    "    for tup in most_frequent:\n",
    "        writer.writerow(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually excluding from this list all names of persons, locations, and organisations, as well as all dates and times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.read_csv('preprocessed_results/most_frequent_tokens_cleaned.csv', header=None, names=['token', 'frequency'])\n",
    "tokens['tfidf'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "cv = CountVectorizer(analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)\n",
    "data = cv.fit_transform(df['tokens'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "# create dictionary to find a tfidf word each word\n",
    "word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, score in word2tfidf.items():\n",
    "    if word in tokens['token'].tolist():\n",
    "        tokens.loc[tokens['token']==word, 'tfidf'] = score\n",
    "        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the frequent tokens for each document\n",
    "def filter_tokens(all_tokens):\n",
    "    return [token for token in all_tokens if token in tokens['token'].tolist()]\n",
    "\n",
    "df['filtered_tokens'] = df['tokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(  #initiating a tfidf vectorizer from list of tokens\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) \n",
    "\n",
    "tokens_transformed = tfidf.fit_transform(df['filtered_tokens']) #fit and transform to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_all,kmo_model=calculate_kmo(tokens_transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = np.zeros((tokens_transformed.shape[0], len(kmo_all)))\n",
    "for i in range(len(kmo_all)):\n",
    "    if kmo_all[i] > 0.5:  #keeping only those that have kmo over 0.5\n",
    "        features_pca[i] = tokens_transformed[i].toarray()[0]\n",
    "    \n",
    "print(len(features_pca), tokens_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running PCA on the filtered tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_pca_scaled = scaler.fit_transform(features_pca)\n",
    "\n",
    "pca_results = {'Num_of_components': [],\n",
    "                'Explained_variance': [],\n",
    "               'Terms':[]\n",
    "                }\n",
    "for n in range (3, 21):\n",
    "    pca_model = (CustomPCA(n_components=n)\n",
    "                    .fit(features_pca_scaled))\n",
    "    pca_results['Num_of_components'].append(n)\n",
    "    pca_results['Explained_variance'].append(sum(pca_model.explained_variance_ratio_))\n",
    "    terms = []\n",
    "    for i in range(n):\n",
    "        terms.append(tokens.token[(pca_model.components_[i].round(1)>0.1) | (pca_model.components_[i].round(1)<-0.1)].tolist())\n",
    "    pca_results['Terms'].append(terms)\n",
    "    \n",
    "pca_results_df = pd.DataFrame(pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_df.to_csv('results/mediacloud_pca_results.csv')\n",
    "pca_results_df.plot.line(x='Num_of_components', y='Explained_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_df['Terms'][17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_pca = PCA().fit(features_pca_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x_values = range(1, cummulative_pca.n_components_+1)\n",
    "ax.plot(x_values, cummulative_pca.explained_variance_ratio_, lw=2, label='explained variance')\n",
    "ax.plot(x_values, np.cumsum(cummulative_pca.explained_variance_ratio_), lw=2, label='cumulative explained variance')\n",
    "ax.set_title('PCA on filtered tokens : explained variance of components')\n",
    "ax.set_xlabel('principal component')\n",
    "ax.set_ylabel('explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df.loc[:,['target']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_transformed.toarray().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
